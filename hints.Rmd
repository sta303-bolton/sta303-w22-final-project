---
title: "Data and hints"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 3
---

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("https://media.istockphoto.com/photos/barrier-under-construction-picture-id1139968862?k=20&m=1139968862&s=612x612&w=0&h=QXAvHsIYHG5MLxMIgIkICw8-n31R3sQvup2d6LGd3eQ=")
```


```{r, xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clone fa-2x\" style=\"color: #301e64\"></i>",
    success_text = "<i class=\"fa fa-check fa-2x\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times fa-2x\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

# Web scraping a website with one simple table

Here is some sample code, using the `rvest` package

```{r, eval = FALSE}
# Note: In adapting this for your code, 
# please ensure all libraries are in a setup chunk at the beginning

# These are the libraries I find useful for webscraping
library(tidyverse)
library(polite)
library(rvest)

url <- "url you are scarping"

# Make sure this code is updated appropriately to provide 
# informative user_agent details
target <- bow(url,
              user_agent = "liza.bolton@utoronto.ca for STA303/1002 project",
              force = TRUE)

# Any details provided in the robots text on crawl delays and 
# which agents are allowed to scrape
target

html <- scrape(target)

device_data <- html %>% 
  html_elements("table") %>% 
  html_table()

```

# Postal code conversion file

- As a university of Toronto student you have access to a Census Canada [Postal Code Conversion Files](https://mdl.library.utoronto.ca/collections/numeric-data/census-canada/postal-code-conversion-file)

- Ethical considerations
  - You are asked to accept a license agreement to get access to this data.
  - This is data that should NOT go directly on to your GitHub. Think carefully about how you will process this data, saving only the information you need for any data that is part of your final submission.
- I recommend downloading the .sav file version. It says it is for SPSS, but it is easy to read into R.
- Choose an appropriate year and make sure you specify/judtify it in your write-up.
  
  
```{r, eval = FALSE}
# install.packages("haven")
library(haven)
dataset = read_sav("prof-data/pccfNat_fccpNat_082021sav.sav")

postcode <- dataset %>% 
  select(PC, CSDuid)

```

# Getting income data from the Canadian census

- Sign up for the cancensus API
- Get your API key and repalce it below

```{r, eval=FALSE}
# install.packages("cancensus")
library(cancensus)


options(cancensus.api_key = "your API key here",
        cancensus.cache_path = "cache") # this sets a folder for your cache


# get all regions as at the 2016 Census (2020 not up yet)
regions <- list_census_regions(dataset = "CA16")

regions_filtered <-  regions %>% 
  filter(level == "CSD") %>% # Figure out what CSD means in Census data
  as_census_region_list()

# This can take a while
census_data_csd <- get_census(dataset='CA16', regions = regions_filtered,
                          vectors=c("v_CA16_2397"),
                          level='CSD', geo_format = "sf")

# Simplify to only needed variables
median_income <- census_data_csd %>% 
  as_tibble() %>% 
  select(CSDuid = GeoUID, contains("median"), Population) %>% 
  mutate(CSDuid = parse_number(CSDuid)) %>% 
  rename(hhld_median_inc = 2)

```



